# spinor72_infra
spinor72 Infra repository
[![Build Status](https://travis-ci.org/Otus-DevOps-2018-02/spinor72_infra.svg?branch=master)](https://travis-ci.org/Otus-DevOps-2018-02/spinor72_infra)

## ДЗ №4
#### Самостоятельное задание (слайд 35)
Подключиться через хост bastion можно, например, такого вида командой:
`ssh -i ~/.ssh/appuser -o ProxyCommand="ssh -i ~/.ssh/appuser -W %h:%p appuser@35.204.145.242" appuser@10.164.0.3`

Для подключения простой командой вида `ssh someinternalhost` нужно воспользоваться возможностями файла конфигурации ssh.
Например, создать файл `~/.ssh/config` (если он еще не создан)
и добавить в него содержимое:
```
Host bastion
  Hostname 35.204.145.242
  User appuser
  IdentityFile /home/spa/.ssh/appuser

Host someinternalhost
  Hostname 10.164.0.3
  User appuser
  IdentityFile /home/spa/.ssh/appuser
  ProxyCommand ssh bastion -W %h:%p
```

#### Параметры для проверки:

bastion_IP = 35.204.145.242

someinternalhost_IP = 10.164.0.3

## ДЗ №5 Деплой тестового приложения

 - [x] Основное ДЗ
 - [x] Дополнительные задания

### В процессе сделано:

 - Создан инстанс reddit-app
 - Установлены пакеты ruby, bundle и mongodb
 - Тестовое приложение загружено из репозитория. 
 - Установлены зависимости
 - Проверена работа тестового приложения
 - Сделаны скрипты , автоматизирующие указанные выше действия 
   - для запуска из инстанса `install_ruby.sh`, `install_mongodb.sh`, `deploy.sh`
   - для автоматического развёртывания `reddit-app-init.sh`, `startup-script.sh` 

### Как запустить проект:
 - Запустить скрипт `reddit-app-init.sh` 

### Как проверить работоспособность:
 - Проверить скриптом `reddit-app-check.sh` ,что сервис отвечает (скрипт должен определеить ip-адрес созданного инстанса и проверить ответ сервера на код 200) 
 - Перейти по ссылке http://35.204.145.242:9292 . Должен открыться сайтик, на котором можно зарегистрироваться, и написать сообщение. 

_После проверки, удалить созданный инстанс и правила файвола можно скриптом `reddit-app-cleanup.sh`_


#### Дополнительное задание. Стартовый скрипт. (слайд 20)
```bash
gcloud compute instances create reddit-app\
  --boot-disk-size=10GB \
  --image-family ubuntu-1604-lts \
  --image-project=ubuntu-os-cloud \
  --machine-type=g1-small \
  --tags puma-server \
  --restart-on-failure \
  --metadata-from-file startup-script=./startup-script.sh
```
#### Дополнительное задание. Правило файволла (слайд 21)
```bash
gcloud compute firewall-rules create default-puma-server\
   --allow=tcp:9292 \
   --description="Access puma server from external network"\
   --source-ranges=0.0.0.0/0\
   --target-tags=puma-server 
```

#### Параметры для проверки:
testapp_IP = 35.204.145.242
testapp_port = 9292

## ДЗ №6 Сборка образов VM при помощи Packer

 - [x] Основное ДЗ
 - [x] Задания со *

### В процессе сделано:
 - скрипты от предыдущего задания перенесены в папку `config-scripts`
 - В папке packer созданы конфигурационные файлы `ubuntu16.json` и `immutable.json` для создания образа ВМ с помощью packer
 - Файлы параметризованы, пример установки параметров в `variables.json.example`
 - В папке `packer/scripts` размещены скрипты для установки ruby, mongodb и деплоя тестового приложения
 - В папке `packer/files` размещен systemd юнит для сервиса тестового приложения
 - Проверена работа утилиты Packer по созданию образа ВМ
 - Проверена работа ВМ созданной на основе  полученного таким способом образа.

### Как запустить проект:
 - перейти в папку packer
 - В папке packer Создать файлик `variables.json` с нужными значениями переменных (за образец взять `variables.json.example`) 
 - Создать образ командой `packer build -var-file=variables.json immutable.json`
 - Создать ВМ скриптом  `config-scripts/create-reddit-vm.sh`

### Как проверить работоспособность:
 - Проверить скриптом `check-reddit-vm.sh` (в папке `config-scripts`) ,что сервис отвечает (скрипт должен определить ip-адрес созданного инстанса и проверить ответ сервера на код 200) 
 - Перейти по ссылке http://<ip-address>:9292 . Должен открыться сайтик, на котором можно зарегистрироваться, и написать сообщение. 

_После проверки, удалить созданный инстанс и правила файвола можно скриптом `config-scripts/delete-reddit-vm.sh`_


## ДЗ №7 Практика IaC с использованием Terraform

 - [x] Основное ДЗ
 - [x] Задания со *
 - [x] Задание с **

### В процессе сделано:
 - Установлен terrafrom
 - В папке репозитории в папке terraform созданы конфигурационные файлы для запуска на основе (подготовленного ранее с помощью packer) базового шаблона ВМ и развертывания тестового приложения. 
 - Продекларированы входные переменные в файле `variables.tf` и выходные в `outputs.tf`
 - Рабочие файлы, генерируемые terraform занесены в список .gitignore
 - В папке `terraform/files` размещен скрипт для развертывания тестового приложения и systemd юнит для соответствующего сервиса
 - Проверена работа terrafrom по созданию и изменению инфраструктуры и корректность работы тестового приложения
 - Для задания со * проверена работа terrafrom по созданию и изменению метаданных проекта на примере ssh ключей. 
   - Если использовать ресурс типа `google_compute_project_metadata_item` то все ключи заменяются terraform-ом  
   - Если использовать ресурс типа `google_compute_project_metadata` то при наличии ранее созданных ключей terrafrom не добавляет ключи
 - Для задания с ** в конфигурацию добавлен балансировщик и проверена корректность его работы. Несколько  инстансов можно создать "вручную" (методом copy-paste) , но лучше использовать параметр `count` , поскольку это предотвращает дублирование кода.

### Как запустить проект:
 - В папке terraform Создать файлик `terrafrom.tfvars` с нужными значениями переменных (за образец взять `terrafrom.tfvars.example`) 
 - Перейти в папку terraform и инициализировать среду командой `terrafrom init`
 - Просмотреть план командой `terrafrom plan` и убедиться в корректности манипуляций, предлагаемых Terraform
 - Запустить процесс создания инфраструктуры командой `terraform apply`
  
### Как проверить работоспособность:
 - С помощью команды `terraform output` узнать ip-адрес балансировщика
 - Открыть в браузере страничку с этим адресом и портом 9292 и проверить работу тестового приложения
 - Подключиться к одному из запущенных инстансов и убедиться что при остановке сервиса puma (`systemctl stop puma`) по прежнему тестовое прилодение доступно на адресе балансировщика, в тоже время на адресе инстанса сервер не отвечает.  

_После проверки, удалить созданное командой `terraform destroy`_

## ДЗ №8 Terraform: ресурсы, модули, окружения и работа в команде

 - [x] Основное ДЗ
 - [x] Задания со *
 
### В процессе сделано:
 - С помощью утилиты packer созданы базовые образы:
    - reddit-db-base для сервера базы данных mongo 
    - reddit-app-base  для сервера приложений на базе ruby
 
 - В конфигурацию terraform импортировано правило файвола для подключения по ssh
 - Добавлен ресурс для присвоения статического адреса серверу тестового приложения
 - Произведено структурирование конфигурации и разделение на модули `app`, `db` и `vpc`, соответствующие конфигурации модулей размещены в подпапках terraform/modules. Добавлены переменные для модулей.
 - Для проверки механизма переиспользования с помощью модулей, настроено два окружения *prod* и *stage* в папках `terraform/prod` и `terraform/stage` соответственно, использующие разные параметры для развертывания (на примере правила файвола)
 - С помощью модуля `SweetOps/storage-bucket/google`   из удаленного реестра созданы бакеты
 - Для задания со *  в окружении prod настроено хранение состояния в бэкэнде Google Cloud Storage и проверена работа блокировки 
 - Для второго задания со * настроен провизионер в отдельном модуле `deploy` для развертывания тестового приложения. 
     В модуль `deploy` передается ip-адрес инстанса на котром должно быть развернуто прилоджение и 
     ip-адрес сервера MongoDB для хранения данных приложения, который прописывается в переменную среды `DATABASE_URL` 
     Модуль db снабжен провизионером для правки конфигурации mongo чтобы разрешить внешние подключения к базе. 
 

### Как запустить проект:
Описание для окружения prod ( stage - аналогично)
 - В папке terraform/prod  Создать файлик `terrafrom.tfvars` с нужными значениями переменных (за образец взять `terraform/prod/terrafrom.tfvars.example`) 
 - Перейти в папку terraform/prod и инициализировать среду командой `terrafrom init`
 - Просмотреть план командой `terrafrom plan` и убедиться в корректности манипуляций, предлагаемых Terraform
 - Запустить процесс создания инфраструктуры командой `terraform apply`
  
### Как проверить работоспособность:
 - С помощью команды `terraform output` узнать ip-адрес сервера с тестовым приложением `app_external_ip`
 - Открыть в браузере страничку с этим адресом и портом 9292 и проверить работу тестового приложения

_После проверки, удалить созданное командой `terraform destroy`_


## ДЗ №9 Знакомство с Ansible.

 - [x] Основное ДЗ
 - [x] Задание со *
 
### В процессе сделано:
 - С помощью terrafrom из окружения stage развернуто 2 тестовых инстанса app и db. 
 - Установлен `ansible`. Настроен конфигурационный файл. Проверен доступ с помощью модуля `ping`
 - Сделаны инвентори в текстовом формате и в YAML-формате
 - Проверена работа модулей  `shell` и `command` и разница в их использовании
 - Проверена работа модуля `git` и его отличие от вызова git в модуле `command`
 - Проверена работа c плейбуками на примере плейбука для клонирования git-репозитория с тестовым приложением. 
 Плейбук восстанавливает клон репозитория, если его полностью удалить. _Если внести изменения в клонированном репозитории, то модуль не выполнится, для принудительного выполнения, нуждно добавить в модуль парамтер `force: true`_
 - Для задания со * сделан инвентори в формате JSON и python-скрипт `json2inventory.py` для вывода в соответствии с документацией https://docs.ansible.com/ansible/latest/dev_guide/developing_inventory.html 

### Как запустить проект:
 Подготовить инфраструктуру:
 В папке terraform/stage  
   - создать файлик `terrafrom.tfvars` с нужными значениями переменных (за образец взять `terraform/stage/terrafrom.tfvars.example`)
   - инициализировать среду командой `terrafrom init` и создать инфраструтуру `terraform apply`
   - узнать ip-адреса тестовых интсансов app и db с помощью `terraform output`.
   - убедиться, что инстанcы доступны по ssh с ключом appuser-а
 
 
 Поправить инвентори-файлы  ansible/inventory и ansible/inventory.yml 
 заменив ip-адреса на соответствующие созданным тестовым инстансам
 
  
### Как проверить работоспособность:
 - Перейти в папку ansible
 - Проверить, что ansible может подключиться к хостам используя разные файлы инвентори командой:  
    - `ansible all -m ping`  для текстового инвентори (имя файла прописано в `ansible.cfg`)
    - `ansible all -m ping -i inventory.yml` для инвентори в формате YAML
  - Проверить работу плейбука командой `ansible-playbook clone.yml` 
  - Для проверки задания со * поменять ip-адреса в файле `inventory.json` и проверить работу командой `ansible all -m ping -i json2inventory.py` 
   
_После проверки, удалить созданные в окружении stage инстансы командой `terraform destroy`_


## ДЗ №10 Деплой и управление конфигурацией с Ansible.

 - [x] Основное ДЗ
 - [x] Задание со *
 
### В процессе сделано:
 - Изменен провижининг в packer. Теперь при создании с помощью packer образов `reddit-app-base` `reddit-db-base` для провижининга используются плейбуки ansible   
 - Рассмотрены несколько подходов к созданию плейбуков и сценариев:
    - `reddit_app_one_play.yml` Один playbook, один сценарий , подход неудобен, особенно при большой инфраструктуре,  поскольку необходимо запускать, явно указывая теги и хосты (`--limit` и `--tags`).
    - `reddit_app_multiple_plays.yml` Один плейбук, несколько сценариев. В этом случае необходимо запускать указывая только теги.
    - `site.yml` Несколько плейбуков , объединныных с помощью include  в главный плейбук. Теги уже не нужны.  Можно переиспользовать плейбуки по-разному их объединяя. 
  - Для удобства формирования inventory после пересоздания инфраструктуры терраформом добавил output-переменную, в которую терраформ запишет в формате yaml данные inventory.
  - Для задания со * написан концепт python-скрипта для динамического инвентори из state-файла терраформа, размещенного в бекенде. Для формирования данных в папку с модулями terrafrom добавлен модуль `ansible`, представляющий из себя null-resource c переменными. В stage-окружении терраформа добавлен удаленный бекенд для хранения конфигурации и конфигурационный файл `ansible.tf` для формирования данных для динамического inventory. 
  
### Как запустить проект:

 Подготовить с помощью packer новые образы, использующие ansible для провижининга, запустив из корня репозитория команды :
 ```
 packer build -var-file=packer/variables.json packer/app.json
 packer build -var-file=packer/variables.json packer/db.json
 ```
_Для того, чтобы провижининг сработал, необходимо чтобы в файволе имелось правило , разрешающее доступ по ssh_
 
 В папке terraform/stage  
   - создать файлик `terrafrom.tfvars` с нужными значениями переменных (за образец взять `terraform/stage/terrafrom.tfvars.example`)
   - инициализировать среду командой `terrafrom init` и создать инфраструктуру `terraform apply`
   - обновить инвентори для ansible `terraform output  inventory_yml > ../../ansible/inventory.yml`
   
_stage теперь хранит стейт в бакете `storage-bucket-spinor-test` его нужно создать, если не создан ранее_

В папке ansible запустить идин из вариантов:
 - Один playbook, один сценарий
  ```
  ansible-playbook reddit_app_one_play.yml --tags db-tag --limit db
  ansible-playbook reddit_app_one_play.yml --tags app-tag --limit app
  ansible-playbook reddit_app_one_play.yml --tags deploy-tag --limit app
  ```
 - Один плейбук, несколько сценариев
  ```
  ansible-playbook reddit_app_multiple_plays.yml --tags db-tag
  ansible-playbook reddit_app_multiple_plays.yml --tags app-tag
  ansible-playbook reddit_app_multiple_plays.yml --tags deploy-tag
  ```
 - Несколько плейбуков , объединныных с помощью include  в главный плейбук
   ```
   ansible-playbook site.yml
   ```
Для пересоздания инфраструктуры, между запусками вариантов, можно использовать скриптик `refresh-infra.sh` :
```
cd ../terraform/stage
terraform destroy -auto-approve=true
terraform apply -auto-approve=true
terraform output  inventory_yml > ../../ansible/inventory.yml
cd ../../ansible
```
#### Задание со *
Для проверки скрипта динамического инвентори поменять в `ansible.cfg` параметр `inventory` на значение `./state2inventory2.py` 
   
### Как проверить работоспособность:
 - Узнать из inventory или terraform output внешний адрес app сервера.
 - Перейти в браузере на страничку с таким адресом и портом 9292 
 - Должен открываться интерфейс тестового приложения, в котором можно залогиниться и написать сообшение. 
   
_После проверки, удалить созданные в окружении stage инстансы командой `terraform destroy`_


## ДЗ №11 Ansible: работа с ролями и окружениями.
[![Build Status](https://travis-ci.org/Otus-DevOps-2018-02/spinor72_infra.svg?branch=ansible-3)](https://travis-ci.org/Otus-DevOps-2018-02/spinor72_infra)

 - [x] Основное ДЗ
 - [x] Задание со *
 - [x] Задание с **
 
### В процессе сделано:
 - В соответствии с принципами ansible best practice реорганизована структура каталогов ansible
 - Созданы роли db и app. Для создания структуры папок роли использована команда `ansible-galaxy init`
 - В плейбуках app и db определение тасков и хендлеров заменены на вызов соответствующих ролей
 - Настроены два окружения - stage и prod, настройки которых размещены в соответсвующих папках `ansible/environments/stage`  и   `ansible/environments/prod`
 - Улучшен файл настроек `ansible.cfg`, по умолчанию настроена работа в `stage` окружении 
 - С помощью комьюнити роли jdauphant.nginx настроен nginx для публикации тестового приложения на 80м порту
 - В модуль app терраформа добавлено правило межсетевого экрана для разрешения доступа к инстансу приложения на 80м tcp порту
 - Добавлен плейбук для создания пользователей. Плейбук зашифрован с помощью ansible vault, чтобы секретные пароли не попали в репозиторий в открытом виде
 - `*` Настроено динамическое инвентори для обоих окружений. Для этого поправлен скрипт state2inventory2.py, такми образом, что он может получать параметры окружения или через переменные среда GS_BUCKET GS_PREFIX или считывать соответствующие параметры из конфигурационного файла state2inventory2.cfg. Для корректной работы нудно поместить в корень соответствующего окружения сам скрипт  state2inventory2.py или симлинк на него. (Оригинал скрипта развещен в stage а симлинк - в prod)  
 - `**` Добавлены проверки в .travis.yml
   - packer validate для всех шаблонов
   - terraform validate и tflint для окружений stage и prod
   - ansible-lint для плейбуков ansible
   - в README.md добавлен бейдж с статусом билда под заголовокм этого ДЗ
   _для отладки настроек Travis использована утилита trytravis_
  
### Как запустить проект:

_Проект использует созданные на предыдущих этапах образы packer и описания инфрастурктуры terraform (см. описания к предыдущему ДЗ)_

 В папке terraform/(stage|prod)  
   - создать файлик `terrafrom.tfvars` с нужными значениями переменных (за образец взять `terraform/(stage|prod)/terrafrom.tfvars.example`)
   - инициализировать среду командой `terrafrom init` и создать инфраструктуру `terraform apply`
 
 - Задать в  конфигурационном файле state2inventory2.cfg  для динамического инвентори в разделе `gs` параметры `bucket` и `prefix` для извлечения из бекенда стейта терраформа.
 - Поместить нужный ключ vault.key в папку `ansible` для расшифровки параметров пользоватлей credentials.yml в окружениях stage и prod. Или использовать свои данные. 
 
 - Запустить из папки `ansible`развертывание командой `ansible-playbook site.yml`  для stage окружения и `ansible-playbook site.yml -i ./environments/prod/state2inventory2.py`
   
### Как проверить работоспособность:
 - Узнать из inventory или terraform output внешний адрес app сервера.
 - Перейти в браузере на страничку с таким адресом и портом 80 
 - Должен открываться интерфейс тестового приложения, в котором можно залогиниться и написать сообщение. 
 - В инстансах app и db должны быть созданы пользователи в соответствии с параметрами `credentials.yml` соответствующего окружения(зайти от имени appuser и проверить список пользователей)
   
_После проверки, удалить созданные в окружении stage|prod инстансы командой `terraform destroy`_

## ДЗ №12 Разработка и тестирование Ansible ролей и плейбуков.
[![Build Status](https://travis-ci.org/Otus-DevOps-2018-02/spinor72_infra.svg?branch=ansible-4)](https://travis-ci.org/Otus-DevOps-2018-02/spinor72_infra)

 - [x] Основное ДЗ
 - [x] Задание со *
 - [x] Задание сo *
 
### В процессе сделано:
Локальное окружение
 - Установлен  `Vagrant` и в `Vagrantfile` описана инфраструткура для локального развертывания 
 - Настроен провижининг Ansible, для работы без предустановленого python использован raw модуль для установки python 
 - Доработаны роли `app` и `db`
   - таски выделены в несколько файлов
   - добавлены теги
   - роли параметризованы 
   - Добавлены `extra_vars` 
     - для пользователя, под которым осуществляется развертывание 
     - для параметров роли nginx (задание * )

Тестирование ролей
 - Установлены `Molecule` и `Testinfra`
 - Настроено локальное тестировоание роли `db` с помощью провайдера vagrant
 - Добавлена проверка, что БД слушает по нужному порту (27017)
 
Обновление плейбуков для packer
 - Роли db и app теперь используются в плейбуках packer_db.yml и packer_app.yml
 - Для корректной генерации образов пакером добавлены в его шаблоны app.json и db.json теги и путь к ролям.

Задание *
 - Для роли db создан отдельный репозиторий https://github.com/spinor72/infra-role-db 
 - Ссылки на репозиторий добавлены в requirements.yml обоих окружений
 - Для созданного репозитория настроен сценарий (`--scenario-name travis`) с помощью gce драйвера Molecule для тестирования роли в облаке GCE
 - Подключен TravisCI для автоматического прогона тестов в GCE 
 - В ридми роли добавлен бейджик со статусом билда
 - Настроено оповещения о билде в слак чат, который использовали в предыдущих ДЗ (https://devops-team-otus.slack.com/messages/C9NT0JMSA/)

  
### Как запустить проект:

Установить Vagrant 
Из папки `ansible` поднять локальное окружение в vagrant командой `vagrant up`

Для настройки тестирования в Molecule установить нужные пакеты , запутсив в папке ansible команду 
`pip install -r requirements.txt`

 
### Как проверить работоспособность:
 - По адресу http://10.10.10.20/ открывается работающий интерфейс тестового приложения 
 - в канал слак приходят сообщения от GitHub и Travic CL относящиеся к репозиторию `spinor72/infra-role-db` 
 - для локальной проверки роли, запустить в корне репозитория https://github.com/spinor72/infra-role-db команды 
 ```
 molecule create 
 molecule converge 
 molecule verify 
 molecule destroy
 ```
 
_После проверки, удалить созданные в локальном окружении инстансы командой `vagrant destroy -f`_
